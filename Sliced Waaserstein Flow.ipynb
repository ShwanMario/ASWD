{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Waaserstein Flow.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOqrw1Fm0nU27OlAa66akG5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"5Jl9cSNpaR9d","colab_type":"code","colab":{}},"source":["import numpy as np\n","import torch\n","from matplotlib import pyplot as plt\n","from torch.autograd import Variable\n","from torch import  optim\n","import ot\n","import ot.gpu\n","from scipy.stats import sem, t\n","from sklearn.datasets import make_moons, make_swiss_roll,make_circles\n","from itertools import combinations\n","import time"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"R3wB0Q2kaefM","colab_type":"code","colab":{}},"source":["def generate_swiss_roll(num_samples):\n","    swiss_roll=make_swiss_roll(n_samples=num_samples, noise=0.0, random_state=None)\n","    swiss_roll=swiss_roll[0][:,[0,2]]\n","    swiss_roll[:,0]=swiss_roll[:,0]/np.abs(swiss_roll[:,0]).max()\n","    swiss_roll[:,1]=swiss_roll[:,1]/np.abs(swiss_roll[:,1]).max()\n","    samples=torch.from_numpy(swiss_roll).type(torch.FloatTensor)\n","    return samples\n","\n","def generate_moons(num_samples):\n","    swiss_roll=make_moons(n_samples=num_samples, noise=0.0, random_state=None)[0]\n","    swiss_roll=swiss_roll/np.abs(swiss_roll).max()\n","    samples=torch.from_numpy(swiss_roll).type(torch.FloatTensor)\n","    return samples\n","def generate_circle(batch_size):\n","\n","    circles = make_circles(2 * batch_size, noise=.01)\n","    z = np.squeeze(circles[0][np.argwhere(circles[1] == 0), :])\n","    return torch.from_numpy(z).type(torch.FloatTensor)\n","def generate_rectangle(batch_size):\n","    z = 2 * (np.random.uniform(size=(batch_size, 2)) - 0.5)\n","    return torch.from_numpy(z).type(torch.FloatTensor)\n","def generate_8gaussian(num_samples):\n","    theta=np.arange(0,2*np.pi,2*np.pi/8,)\n","    samples=np.zeros([num_samples,2])\n","    cov= [[1e-4, 0], [0, 1e-4]]\n","    n,i,k=0,0,0\n","    while n<num_samples:\n","      l=min(60,num_samples-n)\n","      mean=[2**0.5*np.sin(theta[i%8]),2**0.5*np.cos(theta[i%8])]\n","      samples[i*60:i*60+l]=np.random.multivariate_normal(mean, cov, l)\n","      n+=l\n","      i+=1\n","    return torch.from_numpy(samples).type(torch.FloatTensor)\n","def generate_25gaussian(num_samples):\n","  x=np.linspace(-2**0.5,2**0.5,5)\n","  y=np.linspace(-2**0.5,2**0.5,5)\n","  n=num_samples//25\n","  cov= [[1e-4, 0], [0, 1e-4]]\n","  samples=np.zeros([num_samples,2])\n","  index=0\n","  for i in x:\n","    for j in y:\n","      mean=[i,j]\n","      samples[index*n:(index+1)*n]=np.random.multivariate_normal(mean, cov, n)\n","      index+=1\n","  return torch.from_numpy(samples).type(torch.FloatTensor)\n","def generate_knot(num_samples):\n","\n","    l=np.arange(0,2*np.pi,2*np.pi/num_samples,)\n","    x=(np.sin(np.pi*l)*np.cos(l)).reshape((num_samples,1))\n","    y=(np.sin(np.pi*l)*np.sin(l)).reshape((num_samples,-1))\n","    z=np.concatenate((x,y),axis=1)\n","    return torch.from_numpy(z).type(torch.FloatTensor)\n","def generate_heart(num_samples):\n","    t=np.arange(0,2*np.pi,2*np.pi/num_samples,)\n","    a=16*np.sin(t)**3\n","    b=13*np.cos(t)-5*np.cos(2*t)-2*np.cos(3*t)-np.cos(4*t)\n","    samples=np.zeros([num_samples,2])\n","    samples[:,0],samples[:,1]=a,b\n","    samples[:,0]/=np.abs(samples[:,0]).max()\n","    samples[:,1]/=np.abs(samples[:,1]).max(),\n","    return torch.from_numpy(samples).type(torch.FloatTensor)\n","def rand_projections(dim, num_projections=1000):\n","    projections = torch.randn((num_projections, dim))\n","    projections = projections / torch.sqrt(torch.sum(projections ** 2, dim=1, keepdim=True))\n","    return projections\n","def sliced_wasserstein_distance(first_samples,\n","                                second_samples,\n","                                num_projection=1000,\n","                                p=2,\n","                                device='cuda'):\n","    dim = second_samples.size(1)\n","    projections = rand_projections(dim, num_projections).to(device)\n","    first_projections = first_samples.matmul(projections.transpose(0, 1))\n","    second_projections = (second_samples.matmul(projections.transpose(0, 1)))\n","    wasserstein_distance = torch.abs((torch.sort(first_projections.transpose(0, 1), dim=1)[0] -\n","                            torch.sort(second_projections.transpose(0, 1), dim=1)[0]))\n","    wasserstein_distance = torch.pow(torch.sum(torch.pow(wasserstein_distance, p), dim=1),1./p)\n","    return torch.pow(torch.pow(wasserstein_distance, p).mean(),1./p)\n","def max_sliced_wasserstein_distance(first_samples,\n","                                second_samples,\n","                                num_projection=1000,\n","                                p=2,max_iter=10,\n","                                device='cuda'):\n","    dim = second_samples.size(1)\n","    num_projection=1\n","    first_samples_detach = first_samples.detach()\n","    second_samples_detach = second_samples.detach()\n","    projections = rand_projections(dim, 1).to(device)\n","    projections.requires_grad_()\n","    optimizer=optim.Adam([projections], lr=0.005, betas=(0.999, 0.999))\n","    for i in range(max_iter):\n","      first_projections = first_samples_detach.matmul(projections.transpose(0, 1))\n","      second_projections = second_samples_detach.matmul(projections.transpose(0, 1))\n","      wasserstein_distance = torch.abs((torch.sort(first_projections.transpose(0, 1), dim=1)[0] -\n","                              torch.sort(second_projections.transpose(0, 1), dim=1)[0]))\n","      wasserstein_distance = torch.sum(torch.pow(wasserstein_distance, p), dim=1)\n","      wasserstein_distance = torch.pow(wasserstein_distance.mean(),1./p)\n","      optimizer.zero_grad()\n","      loss=-wasserstein_distance\n","      loss.backward(retain_graph=True)\n","      optimizer.step()\n","      projections.data = projections.data / torch.sqrt(torch.sum(projections.data ** 2, dim=1))\n","    first_projections = first_samples.matmul(projections.transpose(0, 1))\n","    second_projections = second_samples.matmul(projections.transpose(0, 1))\n","    wasserstein_distance = torch.abs((torch.sort(first_projections.transpose(0, 1), dim=1)[0] -\n","                                      torch.sort(second_projections.transpose(0, 1), dim=1)[0]))\n","    wasserstein_distance = torch.sum(torch.pow(wasserstein_distance, p), dim=1)\n","    wasserstein_distance = torch.pow(wasserstein_distance.mean(),1./p)\n","    return wasserstein_distance  \n","def poly_degree(degree,dim):\n","    comb=combinations(np.arange(1,degree+dim),dim-1)\n","    comb=list(comb)\n","    x_degree=np.zeros([len(comb),dim])\n","    for (i,c) in enumerate(comb):\n","        c=list(c)\n","        c.append(degree+dim)\n","        for (j,index) in enumerate(c):\n","            if j == 0:\n","                x_degree[i,j]=index-1\n","            else:\n","                x_degree[i,j]=index-c[j-1]-1\n","    return torch.from_numpy(x_degree).type(torch.FloatTensor)\n","def polynomial_function(samples, parameters, device='cuda'):\n","    theta, degree_matrix = parameters[0], parameters[1]\n","    prod_value_poly = torch.zeros(samples.shape[0], degree_matrix.shape[0]).to(device)\n","    degree_matrix = degree_matrix.unsqueeze(0).expand(samples.shape[0], degree_matrix.shape[0],\n","                                                      degree_matrix.shape[1])\n","    samples = samples.unsqueeze(1).expand(samples.shape[0], degree_matrix.shape[1], samples.shape[1])\n","    exp_matrix = samples ** degree_matrix\n","    polynomial_function_value = torch.matmul(torch.prod(exp_matrix, -1), theta)\n","    return polynomial_function_value\n","\n","def max_GSWD_polynomial_3(encoded_samples,distribution_samples,p=2,num_projection=1000,degree=3 ,device='cuda',max_iter=10):\n","    num_projection=1\n","    mat_1 = poly_degree(degree, encoded_samples.shape[-1]).to(device)\n","    coefficient_1 = torch.randn((mat_1.shape[0], num_projection), device=device)\n","    coefficient_1.data = coefficient_1.data / torch.sqrt(torch.sum(coefficient_1.data ** 2, dim=0))\n","    coefficient_1.requires_grad_()\n","    optimizer=optim.Adam([coefficient_1], lr=0.005, betas=(0.999, 0.999))\n","    for i in range(max_iter):\n","      encoded_projections=polynomial_function(encoded_samples.detach(),[coefficient_1,mat_1.detach()])\n","      distribution_projections=polynomial_function(distribution_samples.detach(),[coefficient_1,mat_1.detach()])\n","      wasserstein_distance=torch.abs(torch.sort(encoded_projections.transpose(0,1),dim=1)[0]-\n","                                    torch.sort(distribution_projections.transpose(0,1),dim=1)[0])\n","      wasserstein_distance=torch.sum(torch.pow(wasserstein_distance,p),dim=-1)\n","      wasserstein_distance=torch.pow(torch.mean(wasserstein_distance),1./p)\n","      optimizer.zero_grad()\n","      loss=-wasserstein_distance\n","      loss.backward(retain_graph=True)\n","      optimizer.step()\n","      coefficient_1.data = coefficient_1.data / torch.sqrt(torch.sum(coefficient_1.data ** 2, dim=0))\n","    encoded_projections=polynomial_function(encoded_samples,[coefficient_1.detach(),mat_1.detach()])\n","    distribution_projections=polynomial_function(distribution_samples,[coefficient_1.detach(),mat_1.detach()])\n","    wasserstein_distance=torch.abs(torch.sort(encoded_projections.transpose(0,1),dim=1)[0]-\n","                                   torch.sort(distribution_projections.transpose(0,1),dim=1)[0])\n","    wasserstein_distance=torch.sum(torch.pow(wasserstein_distance,p),dim=-1)\n","    wasserstein_distance=torch.pow(torch.mean(wasserstein_distance),1./p)\n","    return wasserstein_distance\n","\n","def max_GSWD_polynomial_5(encoded_samples,distribution_samples,p=2,num_projection=1000,degree=5 ,device='cuda',max_iter=10):\n","    num_projection=1\n","    mat_1 = poly_degree(degree, encoded_samples.shape[-1]).to(device)\n","    coefficient_1 = torch.randn((mat_1.shape[0], num_projection), device=device, requires_grad=True)\n","    coefficient_1.data = coefficient_1.data / torch.sqrt(torch.sum(coefficient_1.data ** 2, dim=0))\n","    optimizer=optim.Adam([coefficient_1], lr=0.005, betas=(0.999, 0.999))\n","    for i in range(max_iter):\n","      encoded_projections=polynomial_function(encoded_samples.detach(),[coefficient_1,mat_1.detach()])\n","      distribution_projections=polynomial_function(distribution_samples.detach(),[coefficient_1,mat_1.detach()])\n","      wasserstein_distance=torch.abs(torch.sort(encoded_projections.transpose(0,1),dim=1)[0]-\n","                                    torch.sort(distribution_projections.transpose(0,1),dim=1)[0])\n","      wasserstein_distance=torch.sum(torch.pow(wasserstein_distance,p),dim=-1)\n","      wasserstein_distance=torch.pow(torch.mean(wasserstein_distance),1./p)\n","      optimizer.zero_grad()\n","      loss=-wasserstein_distance\n","      loss.backward(retain_graph=True)\n","      optimizer.step()\n","      coefficient_1.data = coefficient_1.data / torch.sqrt(torch.sum(coefficient_1.data ** 2, dim=0))\n","    encoded_projections=polynomial_function(encoded_samples,[coefficient_1.detach(),mat_1.detach()])\n","    distribution_projections=polynomial_function(distribution_samples,[coefficient_1.detach(),mat_1.detach()])\n","    wasserstein_distance=torch.abs(torch.sort(encoded_projections.transpose(0,1),dim=1)[0]-\n","                                   torch.sort(distribution_projections.transpose(0,1),dim=1)[0])\n","    wasserstein_distance=torch.sum(torch.pow(wasserstein_distance,p),dim=-1)\n","    wasserstein_distance=torch.pow(torch.mean(wasserstein_distance),1./p)\n","    return wasserstein_distance\n","\n","def GSWD_polynomial(encoded_samples,distribution_samples,p=2,num_projection=1000,degree=5 ,device='cuda'):\n","    mat_1 = poly_degree(degree, encoded_samples.shape[-1]).to(device)\n","    coefficient_1 = torch.randn((mat_1.shape[0], num_projection), device=device, requires_grad=True)\n","    coefficient_1.data = coefficient_1.data / torch.sqrt(torch.sum(coefficient_1.data ** 2, dim=0))\n","    encoded_projections=polynomial_function(encoded_samples,[coefficient_1.detach(),mat_1.detach()])\n","    distribution_projections=polynomial_function(distribution_samples,[coefficient_1.detach(),mat_1.detach()])\n","    wasserstein_distance=torch.abs(torch.sort(encoded_projections.transpose(0,1),dim=1)[0]-\n","                                   torch.sort(distribution_projections.transpose(0,1),dim=1)[0])\n","    wasserstein_distance=torch.sum(torch.pow(wasserstein_distance,p),dim=-1)\n","    wasserstein_distance=torch.pow(torch.mean(wasserstein_distance),1./p)\n","    return wasserstein_distance\n","\n","def GSWD_circular(encoded_samples,distribution_samples,p=2,num_projection=1000,r=1,device='cuda'):\n","    theta = torch.randn((num_projection, encoded_samples.shape[1])).to(device)\n","    theta = theta / torch.sqrt(torch.sum(theta ** 2, dim=1, keepdim=True))\n","    cost_matrix_1 = torch.sqrt(cost_matrix(encoded_samples, theta * r))\n","    cost_matrix_2 = torch.sqrt(cost_matrix(distribution_samples, theta * r))\n","    wasserstein_distance = torch.abs(torch.sort(cost_matrix_1.transpose(0, 1), dim=1)[0] -\n","                                      torch.sort(cost_matrix_2.transpose(0, 1), dim=1)[0])\n","    wasserstein_distance =torch.sum(torch.pow(wasserstein_distance, p), dim=1)\n","    return torch.pow(wasserstein_distance.mean(), 1. / p)\n","\n","def max_GSWD_circular(encoded_samples,distribution_samples,p=2,num_projection=1,r=1,device='cuda',max_iter=10):\n","    num_projection=1\n","    theta = torch.randn((num_projection, encoded_samples.shape[1])).to(device)\n","    theta.data = theta.data / torch.sqrt(torch.sum(theta.data ** 2, dim=1, keepdim=True))\n","    theta.requires_grad_()\n","    optimizer=optim.Adam([theta], lr=0.005, betas=(0.999, 0.999))\n","\n","    for i in range(max_iter):\n","      cost_matrix_1 = torch.sqrt(cost_matrix(encoded_samples.detach(), theta * r))\n","      cost_matrix_2 = torch.sqrt(cost_matrix(distribution_samples.detach(), theta * r))\n","      wasserstein_distance = torch.abs(torch.sort(cost_matrix_1.transpose(0, 1), dim=1)[0] -\n","                                        torch.sort(cost_matrix_2.transpose(0, 1), dim=1)[0])\n","      wasserstein_distance =torch.sum(torch.pow(wasserstein_distance, p), dim=1)\n","      wasserstein_distance=torch.pow(wasserstein_distance.mean(), 1. / p)\n","      optimizer.zero_grad()\n","      loss=-wasserstein_distance\n","      loss.backward(retain_graph=True)\n","      optimizer.step()\n","      theta.data = theta.data / torch.sqrt(torch.sum(theta.data ** 2, dim=1))\n","    cost_matrix_1 = torch.sqrt(cost_matrix(encoded_samples, theta * r))\n","    cost_matrix_2 = torch.sqrt(cost_matrix(distribution_samples, theta * r))\n","    wasserstein_distance = torch.abs(torch.sort(cost_matrix_1.transpose(0, 1), dim=1)[0] -\n","                                      torch.sort(cost_matrix_2.transpose(0, 1), dim=1)[0])\n","    wasserstein_distance =torch.sum(torch.pow(wasserstein_distance, p), dim=1)\n","    return torch.pow(wasserstein_distance.mean(), 1. / p)\n","\n","def circular_function(samples, parameters):\n","    radial, theta = parameters[0], parameters[1]\n","    cost_matrix_1 = torch.sqrt(cost_matrix(samples, theta * radial))\n","    return cost_matrix_1\n","\n","def cost_matrix(encoded_samples, distribution_samples, p=2):\n","    n = encoded_samples.size(0)\n","    m = distribution_samples.size(0)\n","    d = encoded_samples.size(1)\n","    return ((encoded_samples.reshape(n,d,1)-distribution_samples.transpose(1,0).unsqueeze(0))**2).sum(1)\n","\n","def GSWD_polynomial3(encoded_samples,distribution_samples,p=2,num_projection=1000,device='cuda'):\n","    theta = torch.randn((num_projection, 4)).to(device)\n","    theta = theta / torch.sqrt(torch.sum(theta ** 2, dim=1, keepdim=True))\n","    encoded_samples_=torch.ones(encoded_samples.shape[0],4).to(device)\n","    distribution_samples_=torch.ones(distribution_samples.shape[0],4).to(device)\n","    encoded_samples_[:,0],encoded_samples_[:,1],encoded_samples_[:,2],encoded_samples_[:,3]=encoded_samples[:,1]**3,encoded_samples[:,0]*encoded_samples[:,1]**2,encoded_samples[:,0]**2*encoded_samples[:,1],encoded_samples[:,0]**3\n","    distribution_samples_[:,0],distribution_samples_[:,1],distribution_samples_[:,2],distribution_samples_[:,3]=distribution_samples[:,1]**3,distribution_samples[:,0]*distribution_samples[:,1]**2,distribution_samples[:,0]**2*distribution_samples[:,1]**1,distribution_samples[:,0]**3\n","    encoded_projections=encoded_samples_.matmul(theta.transpose(0,1))\n","    distribution_projections=distribution_samples_.matmul(theta.transpose(0,1))\n","    wasserstein_distance=torch.abs(torch.sort(encoded_projections.transpose(0,1),dim=1)[0]-\n","                                   torch.sort(distribution_projections.transpose(0,1),dim=1)[0])\n","    wasserstein_distance=torch.sum(torch.pow(wasserstein_distance,p),dim=-1)\n","    wasserstein_distance=torch.pow(torch.mean(wasserstein_distance),1./p)\n","    return wasserstein_distance\n","\n","from torch import nn\n","class Mapping(nn.Module):\n","    def __init__(self, size):\n","        super(Mapping, self).__init__()\n","        self.size = size\n","        self.net = nn.Sequential(nn.Linear(self.size, self.size))\n","    def forward(self, inputs):\n","        outputs =self.net(inputs)\n","        return torch.cat((inputs,outputs),dim=-1)\n","\n","def augmented_sliced_wassersten_distance(first_samples,second_samples,num_projections,phi,\n","                                                       phi_op,p=2,max_iter=10,lam=20,device='cuda',net_type='fc'):\n","    embedding_dim = first_samples.size(1)\n","    first_samples_detach = first_samples.detach()\n","    second_samples_detach = second_samples.detach()\n","    for _ in range(max_iter):\n","        first_samples_transform=phi(first_samples_detach)\n","        second_samples_transform = phi(second_samples_detach)\n","        reg=lam*(torch.norm(first_samples_transform,p=2,dim=1)+torch.norm(second_samples_transform,p=2,dim=1)).mean()\n","        projections = rand_projections(first_samples_transform.shape[-1], num_projections).to(device)\n","        encoded_projections = first_samples_transform.matmul(projections.transpose(0, 1))\n","        distribution_projections = (second_samples_transform.matmul(projections.transpose(0, 1)))\n","        wasserstein_distance = torch.abs((torch.sort(encoded_projections.transpose(0, 1), dim=1)[0] -\n","                                torch.sort(distribution_projections.transpose(0, 1), dim=1)[0]))\n","        wasserstein_distance = torch.sum(torch.pow(wasserstein_distance, p), dim=1)*512/first_samples_detach.shape[0]\n","        wasserstein_distance = torch.pow(wasserstein_distance.mean(), 1. / p)\n","        loss=reg-wasserstein_distance\n","        phi_op.zero_grad()\n","        loss.backward(retain_graph=True)\n","        phi_op.step()\n","    first_samples_transform = phi(first_samples)\n","    second_samples_transform = phi(second_samples)\n","    projections=rand_projections(first_samples_transform.shape[-1], num_projections).to(device)\n","    encoded_projections = first_samples_transform.matmul(projections.transpose(0, 1))\n","    distribution_projections = second_samples_transform.matmul(projections.transpose(0, 1))\n","    wasserstein_distance = torch.abs((torch.sort(encoded_projections.transpose(0, 1), dim=1)[0] -\n","                                      torch.sort(distribution_projections.transpose(0, 1), dim=1)[0]))\n","    wasserstein_distance = torch.sum(torch.pow(wasserstein_distance, p), dim=1)\n","    wasserstein_distance = torch.pow(wasserstein_distance.mean(), 1. / p)\n","    return  wasserstein_distance\n","\n","class TransformNet(nn.Module):\n","    def __init__(self, size):\n","        super(TransformNet, self).__init__()\n","        self.size = size\n","        self.net = nn.Sequential(nn.Linear(self.size,self.size))\n","    def forward(self, input):\n","        out =self.net(input)\n","        return out/torch.sqrt(torch.sum(out**2,dim=1,keepdim=True))\n","\n","def cosine_distance_torch(x1, x2=None, eps=1e-8):\n","    x2 = x1 if x2 is None else x2\n","    w1 = x1.norm(p=2, dim=1, keepdim=True)\n","    w2 = w1 if x2 is x1 else x2.norm(p=2, dim=1, keepdim=True)\n","    return torch.mean(torch.abs(torch.mm(x1, x2.t()) / (w1 * w2.t()).clamp(min=eps)))\n","\n","def distributional_sliced_wasserstein_distance(first_samples, second_samples, num_projections, f, f_op,\n","                                               p=2, max_iter=10, lam=1, device='cuda'):\n","    embedding_dim = first_samples.size(1)\n","    pro = rand_projections(embedding_dim, num_projections).to(device)\n","    first_samples_detach = first_samples.detach()\n","    second_samples_detach = second_samples.detach()\n","    for _ in range(max_iter):\n","        \n","        pro = rand_projections(embedding_dim, num_projections).to(device)\n","        projections = f(pro)\n","        reg = lam * cosine_distance_torch(projections, projections)\n","        encoded_projections = first_samples_detach.matmul(projections.transpose(0, 1))\n","        distribution_projections = (second_samples_detach.matmul(projections.transpose(0, 1)))\n","        wasserstein_distance = torch.abs((torch.sort(encoded_projections.transpose(0, 1), dim=1)[0] -\n","                                torch.sort(distribution_projections.transpose(0, 1), dim=1)[0]))\n","        wasserstein_distance = torch.pow(torch.sum(torch.pow(wasserstein_distance, p), dim=1),1./p)\n","        wasserstein_distance = torch.pow(torch.pow(wasserstein_distance, p).mean(),1./p)\n","        loss = reg - wasserstein_distance\n","        f_op.zero_grad()\n","        loss.backward(retain_graph=True)\n","        f_op.step()\n","    pro = rand_projections(embedding_dim, num_projections).to(device)\n","    projections = f(pro)\n","    encoded_projections = first_samples.matmul(projections.transpose(0, 1))\n","    distribution_projections = (second_samples.matmul(projections.transpose(0, 1)))\n","    wasserstein_distance = torch.abs((torch.sort(encoded_projections.transpose(0, 1), dim=1)[0] -\n","                            torch.sort(distribution_projections.transpose(0, 1), dim=1)[0]))\n","    wasserstein_distance = torch.sum(torch.pow(wasserstein_distance, p), dim=1)\n","    wasserstein_distance = torch.pow(wasserstein_distance.mean(), 1. / p)\n","    return wasserstein_distance\n","class MLP(nn.Module):\n","    def __init__(self, din=2,dout=10, num_filters=32, depth=3):\n","        super(MLP, self).__init__()\n","        self.din=din\n","        self.dout=dout\n","        self.init_num_filters = num_filters\n","        self.depth=depth\n","\n","        self.features = nn.Sequential()\n","        \n","        for i in range(self.depth):\n","            if i==0:\n","                self.features.add_module('linear%02d'%(i+1),nn.Linear(self.din,self.init_num_filters))        \n","            else:\n","                self.features.add_module('linear%02d'%(i+1),nn.Linear(self.init_num_filters,self.init_num_filters))\n","            self.features.add_module('activation%02d'%(i+1),nn.LeakyReLU(inplace=True))\n","\n","        self.features.add_module('linear%02d'%(i+2),nn.Linear(self.init_num_filters,self.dout))\n","    \n","    def forward(self, x):        \n","        return self.features(x)\n","    \n","    def init_weights(self,m):\n","        if type(m) == nn.Linear:\n","            nn.init.xavier_uniform_(m.weight)\n","            m.bias.data.fill_(0.01)\n","    \n","    def reset(self):\n","        self.features.apply(self.init_weights)\n","\n","def gsw_nn_1(first_samples, second_samples, net,net_op,max_iter=10,p=2):\n","  encoded_projections=net(first_samples)\n","  distribution_projections=net(second_samples)\n","  wasserstein_distance = torch.abs((torch.sort(encoded_projections.transpose(0, 1), dim=1)[0] -\n","                          torch.sort(distribution_projections.transpose(0, 1), dim=1)[0]))\n","  wasserstein_distance = torch.sum(torch.pow(wasserstein_distance, p), dim=1)\n","  wasserstein_distance = torch.pow(wasserstein_distance.mean(), 1. / p)\n","  return wasserstein_distance\n","\n","def gsw_nn_3(first_samples, second_samples, net,net_op,max_iter=10,p=2):\n","  encoded_projections=net(first_samples)\n","  distribution_projections=net(second_samples)\n","  wasserstein_distance = torch.abs((torch.sort(encoded_projections.transpose(0, 1), dim=1)[0] -\n","                          torch.sort(distribution_projections.transpose(0, 1), dim=1)[0]))\n","  wasserstein_distance = torch.sum(torch.pow(wasserstein_distance, p), dim=1)\n","  wasserstein_distance = torch.pow(wasserstein_distance.mean(), 1. / p)\n","  return wasserstein_distance\n","\n","def max_gsw_nn_1(first_samples, second_samples, net, net_op, max_iter=10,p=2):\n","\n","  for i in range(max_iter):\n","    encoded_projections=net(first_samples.detach())\n","    distribution_projections=net(second_samples.detach())\n","    wasserstein_distance = torch.abs((torch.sort(encoded_projections.transpose(0, 1), dim=1)[0] -\n","                            torch.sort(distribution_projections.transpose(0, 1), dim=1)[0]))\n","    wasserstein_distance = torch.sum(torch.pow(wasserstein_distance, p), dim=1)\n","    wasserstein_distance = torch.pow(wasserstein_distance.mean(), 1. / p)\n","    loss=-wasserstein_distance\n","    net_op.zero_grad()\n","    loss.backward(retain_graph=True)\n","    net_op.step()\n","  encoded_projections=net(first_samples)\n","  distribution_projections=net(second_samples)\n","  wasserstein_distance = torch.abs((torch.sort(encoded_projections.transpose(0, 1), dim=1)[0] -\n","                          torch.sort(distribution_projections.transpose(0, 1), dim=1)[0]))\n","  wasserstein_distance = torch.sum(torch.pow(wasserstein_distance, p), dim=1)\n","  wasserstein_distance = torch.pow(wasserstein_distance.mean(), 1. / p)\n","  return wasserstein_distance\n","\n","def max_gsw_nn_3(first_samples, second_samples, net, net_op, max_iter=10,p=2):\n","\n","  for i in range(max_iter):\n","    encoded_projections=net(first_samples.detach())\n","    distribution_projections=net(second_samples.detach())\n","    wasserstein_distance = torch.abs((torch.sort(encoded_projections.transpose(0, 1), dim=1)[0] -\n","                            torch.sort(distribution_projections.transpose(0, 1), dim=1)[0]))\n","    wasserstein_distance = torch.sum(torch.pow(wasserstein_distance, p), dim=1)\n","    wasserstein_distance = torch.pow(wasserstein_distance.mean(), 1. / p)\n","    loss=-wasserstein_distance\n","    net_op.zero_grad()\n","    loss.backward(retain_graph=True)\n","    net_op.step()\n","  encoded_projections=net(first_samples)\n","  distribution_projections=net(second_samples)\n","  wasserstein_distance = torch.abs((torch.sort(encoded_projections.transpose(0, 1), dim=1)[0] -\n","                          torch.sort(distribution_projections.transpose(0, 1), dim=1)[0]))\n","  wasserstein_distance = torch.sum(torch.pow(wasserstein_distance, p), dim=1)\n","  wasserstein_distance = torch.pow(wasserstein_distance.mean(), 1. / p)\n","  return wasserstein_distance\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0THcuEIVaxQ6","colab_type":"code","colab":{}},"source":["n=1\n","num_projections=1000\n","num_iteration=2000\n","num_experiments=50\n","interval=5\n","lr=0.002\n","device='cuda'\n","functions=[augmented_sliced_wassersten_distance,sliced_wasserstein_distance,GSWD_polynomial3,GSWD_polynomial,GSWD_circular,\n","           distributional_sliced_wasserstein_distance,gsw_nn_1,max_gsw_nn_1,gsw_nn_3,max_gsw_nn_3]\n","generators=[generate_swiss_roll,generate_circle,generate_8gaussian,generate_moons,generate_knot,generate_heart,generate_rectangle,generate_25gaussian]\n","\n","dataset=['Swiss','Circle','8gaussian','Moon','Knot','Heart','Rectangle','25gaussian']\n","\n","for d,generator in enumerate(generators):\n","\n","  W2_recorder=np.zeros([len(functions),num_experiments,num_iteration//interval+1])\n","  for k in range(len(functions)):\n","    function=functions[k]\n","    if function==augmented_sliced_wassersten_distance:\n","      for j in range(num_experiments):\n","        target_distribution=generator(500).to('cuda')\n","        lam=0.05/target_distribution.abs().mean()\n","        evolving_distribution=torch.randn_like(target_distribution)\n","        evolving_distribution=Variable(evolving_distribution,requires_grad=True).to('cuda')\n","        optimizer=optim.Adam([evolving_distribution], lr=lr ,betas=(0.9, 0.999))\n","        phi=Mapping(2).to(device)\n","        phi_op = optim.Adam(phi.parameters(), lr=0.005, betas=(0.999, 0.999))\n","        for i in range(num_iteration):\n","          optimizer.zero_grad()\n","\n","          loss=augmented_sliced_wassersten_distance(evolving_distribution,target_distribution,num_projections,phi,\n","                                                                  phi_op,p=2,max_iter=10,lam=lam,device='cuda',net_type='fc')\n","          loss.backward(retain_graph=True)\n","          optimizer.step()\n","          if (i+1)%interval==0 or i==0:\n","            M=(((evolving_distribution.unsqueeze(2)-target_distribution.transpose(1,0).unsqueeze(0))**2).sum(1)).cpu().detach().numpy()\n","            ed1,ed2=np.ones((evolving_distribution.shape[0],))/evolving_distribution.shape[0],np.ones((evolving_distribution.shape[0],))/evolving_distribution.shape[0]\n","            W2=ot.emd2(ed1,ed2,M)**0.5\n","            if i==0:\n","              W2_recorder[k,j,i]=W2\n","            if (i+1)%interval==0:\n","              W2_recorder[k,j,(i+1)//interval]=W2\n","        if j%10==0:\n","          plt.scatter(evolving_distribution.cpu().detach().numpy()[:,0],evolving_distribution.cpu().detach().numpy()[:,1])\n","          plt.show()\n","        print(functions[k].__name__,j,W2)\n","    elif function==distributional_sliced_wasserstein_distance:\n","      for j in range(num_experiments):\n","        target_distribution=generator(500).to('cuda')\n","        lam=10\n","        evolving_distribution=torch.randn_like(target_distribution)\n","        evolving_distribution=Variable(evolving_distribution,requires_grad=True).to('cuda')\n","        optimizer=optim.Adam([evolving_distribution], lr=lr ,betas=(0.9, 0.999))\n","        transform_net = TransformNet(2).to(device)\n","        op_trannet = optim.Adam(transform_net.parameters(), lr=0.005, betas=(0.999, 0.999))\n","        for i in range(num_iteration):\n","          optimizer.zero_grad()\n","          loss=function(evolving_distribution,target_distribution, num_projections, transform_net,\n","                                                           op_trannet,p=2, max_iter=10, lam=lam,device='cuda')\n","          loss.backward(retain_graph=True)\n","          optimizer.step()\n","          if (i+1)%interval==0 or i==0:\n","            M=(((evolving_distribution.unsqueeze(2)-target_distribution.transpose(1,0).unsqueeze(0))**2).sum(1)).cpu().detach().numpy()\n","            ed1,ed2=np.ones((evolving_distribution.shape[0],))/evolving_distribution.shape[0],np.ones((evolving_distribution.shape[0],))/evolving_distribution.shape[0]\n","            W2=ot.emd2(ed1,ed2,M)**0.5\n","            if i==0:\n","              W2_recorder[k,j,i]=W2\n","            if (i+1)%interval==0:\n","              W2_recorder[k,j,(i+1)//interval]=W2\n","        if j%10==0:\n","          plt.scatter(evolving_distribution.cpu().detach().numpy()[:,0],evolving_distribution.cpu().detach().numpy()[:,1])\n","          plt.show()\n","        print(functions[k].__name__,j,W2)\n","    elif function==max_gsw_nn_3 or function==gsw_nn_3 or function==max_gsw_nn_1 or function==gsw_nn_1:\n","      for j in range(num_experiments):\n","        target_distribution=generator(500).to('cuda')\n","        evolving_distribution=torch.randn_like(target_distribution)\n","        evolving_distribution=Variable(evolving_distribution,requires_grad=True).to('cuda')\n","        optimizer=optim.Adam([evolving_distribution], lr=lr ,betas=(0.9, 0.999))\n","        if function==max_gsw_nn_1 or function==gsw_nn_1:\n","          net = MLP(din=2,dout=num_projections,num_filters=32,depth=1).to(device)\n","        elif function==gsw_nn_3 or function==max_gsw_nn_3:\n","          net = MLP(din=2,dout=num_projections,num_filters=32,depth=3).to(device)\n","        net_op = optim.Adam(net.parameters(), lr=0.005,betas=(0.999, 0.999))\n","        for i in range(num_iteration):\n","          optimizer.zero_grad()\n","          loss=function(evolving_distribution,target_distribution, net, net_op,max_iter=10)\n","          loss.backward(retain_graph=True)\n","          optimizer.step()\n","          if (i+1)%interval==0 or i==0:\n","            M=(((evolving_distribution.unsqueeze(2)-target_distribution.transpose(1,0).unsqueeze(0))**2).sum(1)).cpu().detach().numpy()\n","            ed1,ed2=np.ones((evolving_distribution.shape[0],))/evolving_distribution.shape[0],np.ones((evolving_distribution.shape[0],))/evolving_distribution.shape[0]\n","            W2=ot.emd2(ed1,ed2,M)**0.5\n","            if i==0:\n","              W2_recorder[k,j,i]=W2\n","            if (i+1)%interval==0:\n","              W2_recorder[k,j,(i+1)//interval]=W2\n","        if j%10==0:\n","          plt.scatter(evolving_distribution.cpu().detach().numpy()[:,0],evolving_distribution.cpu().detach().numpy()[:,1])\n","          plt.show()\n","        print(functions[k].__name__,j,W2)\n","    else:\n","      for j in range(num_experiments):\n","        target_distribution=generator(500).to('cuda')\n","        evolving_distribution=torch.randn_like(target_distribution)\n","        evolving_distribution=Variable(evolving_distribution,requires_grad=True).to('cuda')\n","        optimizer=optim.Adam([evolving_distribution], lr=lr, betas=(0.9, 0.999))\n","        for i in range(num_iteration):\n","          optimizer.zero_grad()\n","          loss=function(evolving_distribution,target_distribution,num_projection=num_projections)\n","          loss.backward(retain_graph=True)\n","          optimizer.step()\n","          if (i+1)%interval==0 or i==0:\n","            M=(((evolving_distribution.unsqueeze(2)-target_distribution.transpose(1,0).unsqueeze(0))**2).sum(1)).cpu().detach().numpy()\n","            ed1,ed2=np.ones((evolving_distribution.shape[0],))/evolving_distribution.shape[0],np.ones((evolving_distribution.shape[0],))/evolving_distribution.shape[0]\n","            W2=ot.emd2(ed1,ed2,M)**0.5\n","            if i==0:\n","              W2_recorder[k,j,i]=W2\n","            if (i+1)%interval==0:\n","              W2_recorder[k,j,(i+1)//interval]=W2\n","        print(functions[k].__name__,j,W2)\n","        if j%10==0:\n","          plt.scatter(evolving_distribution.cpu().detach().numpy()[:,0],evolving_distribution.cpu().detach().numpy()[:,1])\n","          plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"p1B4seOPozeG","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}