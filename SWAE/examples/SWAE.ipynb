{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of SWAE.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"nFwCuIS9Xywn"},"source":["import torch\n","import torch.optim as optim\n","import torch.nn.functional as F\n","\n","from torch.distributions import MultivariateNormal,Uniform\n","from google.colab import drive\n","\n","import scipy.io\n","import numpy as np\n","import itertools\n","import logging\n","import matplotlib.pyplot as plt\n","torch.set_printoptions(sci_mode=False)\n","from torch import nn\n","import sys"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vL8iNRmGfeoD"},"source":["import argparse\n","import os\n","\n","import matplotlib as mpl\n","#mpl.use('Agg')\n","import matplotlib.pyplot as plt\n","import torch\n","import torch.optim as optim\n","import torchvision.utils as vutils\n","from swae.distributions import rand_cirlce2d, rand_ring2d, rand_uniform2d\n","from swae.models.mnist import MNISTAutoencoder\n","from swae.trainer import SWAEBatchTrainer,Mapping\n","from torchvision import datasets, transforms"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bK_r58RyhVLI"},"source":["parser = argparse.ArgumentParser(description='Sliced Wasserstein Autoencoder PyTorch MNIST Example')\n","parser.add_argument('--datadir', default='/input/', help='path to dataset')\n","parser.add_argument('--outdir', default='/output/', help='directory to output images and model checkpoints')\n","parser.add_argument('--batch-size', type=int, default=500, metavar='N',\n","                    help='input batch size for training (default: 500)')\n","parser.add_argument('--epochs', type=int, default=30, metavar='N',\n","                    help='number of epochs to train (default: 30)')\n","parser.add_argument('--lr', type=float, default=0.001, metavar='LR',\n","                    help='learning rate (default: 0.001)')\n","parser.add_argument('--alpha', type=float, default=0.9, metavar='A',\n","                    help='RMSprop alpha/rho (default: 0.9)')\n","parser.add_argument('--distribution', type=str, default='ring', metavar='DIST',\n","                    help='Latent Distribution (default: circle)')\n","parser.add_argument('--no-cuda', action='store_true', default=False,\n","                    help='disables CUDA training')\n","parser.add_argument('--num-workers', type=int, default=8, metavar='N',\n","                    help='number of dataloader workers if device is CPU (default: 8)')\n","parser.add_argument('--seed', type=int, default=7, metavar='S',\n","                    help='random seed (default: 7)')\n","parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n","                    help='number of batches to log training status (default: 10)')\n","args = parser.parse_args()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Oxva2AgwhdPA"},"source":["bce_recorder=np.zeros([10,30,13])\n","wd_recorder=np.zeros([10,30,13])\n","for i in range(1):\n","  imagesdir = os.path.join(args.outdir, 'images')\n","  chkptdir = os.path.join(args.outdir, 'models')\n","  os.makedirs(args.datadir, exist_ok=True)\n","  os.makedirs(imagesdir, exist_ok=True)\n","  os.makedirs(chkptdir, exist_ok=True)\n","  # determine device and device dep. args\n","  use_cuda = not args.no_cuda and torch.cuda.is_available()\n","  device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","  dataloader_kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {'num_workers': args.num_workers, 'pin_memory': False}\n","  # set random seed\n","  torch.manual_seed(args.seed)\n","  if use_cuda:\n","      torch.cuda.manual_seed(args.seed)\n","  # log args\n","  print('batch size {}\\nepochs {}\\nRMSprop lr {} alpha {}\\ndistribution {}\\nusing device {}\\nseed set to {}'.format(\n","      args.batch_size, args.epochs, args.lr, args.alpha, args.distribution, device.type, args.seed\n","  ))\n","  # build train and test set data loaders\n","  train_loader = torch.utils.data.DataLoader(\n","      datasets.MNIST(args.datadir, train=True, download=True,\n","                      transform=transforms.Compose([\n","                          transforms.ToTensor(),\n","                          transforms.Normalize((0.1307,), (0.3081,))\n","                      ])),\n","      batch_size=args.batch_size, shuffle=True, **dataloader_kwargs)\n","  test_loader = torch.utils.data.DataLoader(\n","      datasets.MNIST(args.datadir, train=False, download=True,\n","                      transform=transforms.Compose([\n","                          transforms.ToTensor(),\n","                          transforms.Normalize((0.1307,), (0.3081,))\n","                      ])),\n","      batch_size=64, shuffle=False, **dataloader_kwargs)\n","  # create encoder and decoder\n","  model = MNISTAutoencoder().to(device)\n","  print(model)\n","  # create optimizer\n","  # matching default Keras args for RMSprop\n","  optimizer = optim.RMSprop(model.parameters(), lr=args.lr, alpha=args.alpha)\n","  # determine latent distribution\n","  if args.distribution == 'circle':\n","      distribution_fn = rand_cirlce2d\n","  elif args.distribution == 'ring':\n","      distribution_fn = rand_ring2d\n","  else:\n","      distribution_fn = rand_uniform2d\n","  mode='saswd'\n","  trainer = SWAEBatchTrainer(model, optimizer, distribution_fn, device=device,mode=mode,num_projections=5)\n","  # put networks in training mode\n","  model.train()\n","  # train networks for n epochs\n","  print('training...')\n","\n","  phi_ = Mapping(28*28+2).cuda()\n","  phi_op_ = optim.Adam(phi_.parameters(), lr=0.001, betas=(0.5, 0.999))\n","\n","  phi = Mapping(2).cuda()\n","  phi_op = optim.Adam(phi.parameters(), lr=0.001, betas=(0.5, 0.999))\n","\n","  trainer.weight=0.1\n","  for epoch in range(args.epochs):\n","    if epoch > 10:\n","        trainer.weight *= 1.05\n","  # train autoencoder on train dataset\n","    for batch_idx, (x, y) in enumerate(train_loader, start=0):\n","        batch = trainer.train_on_batch(x,phi=phi,phi_op=phi_op,max_iter=5,lam=5,phi_=phi_,phi_op_=phi_op_)\n","        if (batch_idx + 1) % args.log_interval == 0 or batch_idx==0:\n","            print('Train Epoch: {} ({:.2f}%) [{}/{}]\\tLoss: {:.6f}'.format(\n","                    epoch + 1, float(epoch + 1) / (args.epochs) * 100.,\n","                    (batch_idx + 1), len(train_loader),\n","                    batch['loss'].item()))\n","            print(batch['bce'].item(),batch['l1'].item(),batch['w2'])\n","            bce_recorder[i,epoch,(batch_idx + 1) // args.log_interval]=batch['bce'].detach().cpu().numpy()\n","            wd_recorder[i,epoch,(batch_idx + 1) // args.log_interval]=batch['w2']\n","  # evaluate autoencoder on test dataset\n","    test_encode, test_targets, test_loss = list(), list(), 0.0\n","    with torch.no_grad():\n","        for test_batch_idx, (x_test, y_test) in enumerate(test_loader, start=0):\n","            test_evals = trainer.test_on_batch(x_test,mode='normal',phi=phi, phi_op=phi_op,)\n","            test_encode.append(test_evals['encode'].detach())\n","            test_loss += test_evals['loss'].item()\n","            test_targets.append(y_test)\n","    test_encode, test_targets = torch.cat(test_encode).cpu().numpy(), torch.cat(test_targets).cpu().numpy()\n","    test_loss /= len(test_loader)\n","    print('Test Epoch: {} ({:.2f}%)\\tLoss: {:.6f}'.format(\n","            epoch + 1, float(epoch + 1) / (args.epochs) * 100.,\n","            test_loss))\n","    print('{{\"metric\": \"loss\", \"value\": {}}}'.format(test_loss))\n","    # save model\n","    torch.save(model.state_dict(), '{}/mnist_epoch_{}.pth'.format(chkptdir, epoch + 1))\n","    # save encoded samples plot\n","    plt.figure(figsize=(10, 10))\n","    plt.scatter(test_encode[:, 0], -test_encode[:, 1], c=(10 * test_targets), cmap=plt.cm.Spectral)\n","    plt.xlim([-1.5, 1.5])\n","    plt.ylim([-1.5, 1.5])\n","    plt.title('Test Latent Space\\nLoss: {:.5f}'.format(test_loss))\n","    plt.savefig('{}/test_latent_epoch_{}_aswd.png'.format(imagesdir, epoch + 1))\n","    plt.show()\n","    \n","    \n","    # save sample input and reconstruction\n","    vutils.save_image(x, '{}/test_samples_epoch_{}_aswd.png'.format(imagesdir, epoch + 1))\n","    vutils.save_image(batch['decode'].detach(),\n","                      '{}/test_reconstructions_epoch_{}_aswd.png'.format(imagesdir, epoch + 1),\n","                      normalize=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0OEo6kTJI81D"},"source":[""],"execution_count":null,"outputs":[]}]}